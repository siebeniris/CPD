{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rootpath\n",
    "import pandas as pd\n",
    "from joblib import Parallel , delayed\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/yiyi/Documents/masterarbeit/CPD'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = rootpath.detect()\n",
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir = os.path.join(root, 'data/sentiment_analysis/results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spellchecked_dir = os.path.join(root, 'data/spellchecked')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/yiyi/Documents/masterarbeit/CPD/data/sentiment_analysis/results'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "count =0 \n",
    "filename = '422#793f568b-3393-4e81-a940-6b2a84ca44bb'\n",
    "\n",
    "filepath = os.path.join(data_dir, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "renovations = [\n",
    "    'build',\n",
    "    'carpeting',\n",
    "    'caulking',\n",
    "    'centerpiece',\n",
    "    'construction',\n",
    "    'damage',\n",
    "    'date',\n",
    "    'dated',\n",
    "    'decor',\n",
    "    'dire',\n",
    "    'décor',\n",
    "    'exterior',\n",
    "    'facelift',\n",
    "    'fix',\n",
    "    'furnish',\n",
    "    'furnishing',\n",
    "    'granite',\n",
    "    'grout',\n",
    "    'improvement',\n",
    "    'maintenance',\n",
    "    'makeover',\n",
    "    'modernisation',\n",
    "    'outdated',\n",
    "    'overhaul',\n",
    "    'paint',\n",
    "    'painting',\n",
    "    'plumbing',\n",
    "    'reconstruction',\n",
    "    'redecorating',\n",
    "    'redo',\n",
    "    'redone',\n",
    "    'refurb',\n",
    "    'refurbish',\n",
    "    'refurbishment',\n",
    "    'remodel',\n",
    "    'remodeling',\n",
    "    'reno',\n",
    "    'renovate',\n",
    "    'renovated',\n",
    "    'renovation',\n",
    "    'repair',\n",
    "    'replace',\n",
    "    'replacement',\n",
    "    'reset',\n",
    "    'rust',\n",
    "    'spruce',\n",
    "    'stylish',\n",
    "    'tape',\n",
    "    'tlc',\n",
    "    'update',\n",
    "    'updating',\n",
    "    'upgrading',\n",
    "    'worn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant\n",
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell= SpellChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>lemma</th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>date</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3a68d8ea-6f20-48a2-a3a0-d0eb4a4c04f8</td>\n",
       "      <td>read review book hotel come prepare disappoint...</td>\n",
       "      <td>I read the reviews before booking this hotel a...</td>\n",
       "      <td>-0.117656</td>\n",
       "      <td>2008-09-01 00:00:00</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3a68d8ea-6f20-48a2-a3a0-d0eb4a4c04f8</td>\n",
       "      <td>room small</td>\n",
       "      <td>The room was extremely small</td>\n",
       "      <td>-0.487555</td>\n",
       "      <td>2008-09-01 00:00:00</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    uid  \\\n",
       "0  3a68d8ea-6f20-48a2-a3a0-d0eb4a4c04f8   \n",
       "1  3a68d8ea-6f20-48a2-a3a0-d0eb4a4c04f8   \n",
       "\n",
       "                                               lemma  \\\n",
       "0  read review book hotel come prepare disappoint...   \n",
       "1                                         room small   \n",
       "\n",
       "                                            sentence  sentiment  \\\n",
       "0  I read the reviews before booking this hotel a...  -0.117656   \n",
       "1                       The room was extremely small  -0.487555   \n",
       "\n",
       "                  date  score  \n",
       "0  2008-09-01 00:00:00   40.0  \n",
       "1  2008-09-01 00:00:00   40.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = df['date'].astype('datetime64')\n",
    "df = df[df['date']>= np.datetime64(\"2015-01-01\")]\n",
    "df['lemma']= df['lemma'].astype(str)\n",
    "lemmas = [lemma.lower().split() for lemma in df['lemma']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4199"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrected_lemmas= Parallel(n_jobs=-1)(delayed(spell_check)(x) for x in lemmas)\n",
    "corrected_lemmas = [[spell.correction(lemma) for lemma in x] for x in lemmas]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(corrected_lemmas,size=200, window=6, min_count=2, workers=8, iter=200, negative=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yiyi/anaconda3/envs/cdp/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('bedroom', 0.6694496870040894),\n",
       " ('gross', 0.5565172433853149),\n",
       " ('hotelscom', 0.5559185743331909),\n",
       " ('pristine', 0.5384526252746582),\n",
       " ('setup', 0.5165766477584839),\n",
       " ('appoint', 0.5075203776359558),\n",
       " ('picture', 0.5032559633255005),\n",
       " ('reserve', 0.49055930972099304),\n",
       " ('upgrade', 0.4787270426750183),\n",
       " ('rust', 0.47131267189979553)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('suite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1458"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vectors.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to_ /home/yiyi/Documents/masterarbeit/CPD/data/spellchecked/267#0ed315c4-9b7b-4423-a320-04cdca7d97ad\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-bed103c39cfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlemma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lemma'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# lemmas_corrected= Parallel(n_jobs=-1)(delayed(spell_check)(x) for x in lemmas)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mlemmas_corrected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlemma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlemmas\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lemma'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemmas_corrected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-bed103c39cfd>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlemma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lemma'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# lemmas_corrected= Parallel(n_jobs=-1)(delayed(spell_check)(x) for x in lemmas)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mlemmas_corrected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlemma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlemmas\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lemma'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemmas_corrected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-bed103c39cfd>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlemma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lemma'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# lemmas_corrected= Parallel(n_jobs=-1)(delayed(spell_check)(x) for x in lemmas)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mlemmas_corrected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlemma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlemmas\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lemma'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemmas_corrected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cdp/lib/python3.7/site-packages/spellchecker/spellchecker.py\u001b[0m in \u001b[0;36mcorrection\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    146\u001b[0m                 str: The most likely candidate \"\"\"\n\u001b[1;32m    147\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mENSURE_UNICODE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_probability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cdp/lib/python3.7/site-packages/spellchecker/spellchecker.py\u001b[0m in \u001b[0;36mcandidates\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m# if still not found, use the edit distance 1 to calc edit distance 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distance\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mknown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__edit_distance_alt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cdp/lib/python3.7/site-packages/spellchecker/spellchecker.py\u001b[0m in \u001b[0;36m__edit_distance_alt\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_if_should_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         ]\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0me2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtmp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mknown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medit_distance_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_if_should_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cdp/lib/python3.7/site-packages/spellchecker/spellchecker.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_if_should_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         ]\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0me2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtmp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mknown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medit_distance_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_if_should_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cdp/lib/python3.7/site-packages/spellchecker/spellchecker.py\u001b[0m in \u001b[0;36mknown\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0mset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mset\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthose\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                 corpus \"\"\"\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mENSURE_UNICODE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_case_sensitive\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         return set(\n",
      "\u001b[0;32m~/anaconda3/envs/cdp/lib/python3.7/site-packages/spellchecker/spellchecker.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0mset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mset\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthose\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                 corpus \"\"\"\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mENSURE_UNICODE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_case_sensitive\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         return set(\n",
      "\u001b[0;32m~/anaconda3/envs/cdp/lib/python3.7/site-packages/spellchecker/utils.py\u001b[0m in \u001b[0;36mENSURE_UNICODE\u001b[0;34m(s, encoding)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mENSURE_UNICODE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "count =0 \n",
    "for filename in os.listdir(data_dir):\n",
    "    filepath = os.path.join(data_dir, filename)\n",
    "    df = pd.read_csv(filepath)\n",
    "    output_path = os.path.join(spellchecked_dir, filename)\n",
    "    \n",
    "    df['date'] = df['date'].astype('datetime64')\n",
    "    df = df[df['date']>= np.datetime64(\"2015-01-01\")]\n",
    "    df['lemma']= df['lemma'].astype(str)\n",
    "    lemmas = [lemma.lower().split() for lemma in df['lemma']]\n",
    "    # lemmas_corrected= Parallel(n_jobs=-1)(delayed(spell_check)(x) for x in lemmas)\n",
    "    lemmas_corrected = [[spell.correction(lemma) for lemma in x] for x in lemmas]\n",
    "    \n",
    "    df['lemma'] = lemmas_corrected\n",
    "    df.to_csv(output_path)\n",
    "    print('save to_', output_path)\n",
    "    model.build_vocab(lemmas_corrected, update=True)\n",
    "    model.train(lemmas_corrected, total_examples= len(lemmas_corrected), epochs =2)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [['room','suite', 'minibar', 'bathroom', 'view', 'shower', 'wifi', \n",
    "             'balcony', 'tv', 'furniture', 'bathtub'],\n",
    "            ['restaurant', 'menu', 'dinner', 'breakfast', 'lunch', \n",
    "             'bar', 'café','dining', 'dish','snack', 'dessert'],\n",
    "            ['facility', 'amenity', 'elevator', 'lobby'],\n",
    "            ['pool','swim'],\n",
    "            ['family', 'vacation'],\n",
    "            ['price', 'cost'],\n",
    "            ['atmosphere', 'ambience', 'vibe', 'experience'],\n",
    "            ['fitness', 'spa', 'sport', 'golf', 'gym'],\n",
    "            ['transport', 'shuttle', 'location'],       \n",
    "            ['beach', 'sea', 'ocean', 'lake'], \n",
    "            ['casino', 'entertainment'],\n",
    "            ['venue', 'meeting', 'event', 'ballroom'],\n",
    "            [ 'parking', 'park'],\n",
    "            ['reception', 'service', 'staff', 'concierge'],\n",
    "            [ 'renovation','revamp', 'overhaul', 'remodel', 'update', 'redone', 'redo',\n",
    "                 'remodel', 'renovate', 'redecorate',\n",
    "                 'refurbish', 'repair', 'refit', 'recondition',\n",
    "                 'renew', 'renewal', 'reform']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "reno_keywords = ['revamp', 'overhaul', 'remodel', 'update', 'redone', 'redo',\n",
    "                 'remodel', 'renovation', 'renovate', 'redecorate',\n",
    "                 'refurbish', 'repair', 'refit', 'recondition',\n",
    "                 'renew', 'renewal', 'reform']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yiyi/anaconda3/envs/cpd/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "renos =[]\n",
    "for word in reno_keywords:\n",
    "    try:\n",
    "        renos.append(list([x for x,y in model.most_similar(word, topn=20 )]))\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "renos_ = list(set(list(chain.from_iterable(renos))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1970',\n",
       " 'anniv',\n",
       " 'bride',\n",
       " 'bridesmaid',\n",
       " 'broke',\n",
       " 'byli',\n",
       " 'caulking',\n",
       " 'construction',\n",
       " 'dateing',\n",
       " 'deerbrooke',\n",
       " 'demolishment',\n",
       " 'desperatley',\n",
       " 'engagement',\n",
       " 'everyrthe',\n",
       " 'expansion',\n",
       " 'facelift',\n",
       " 'fix',\n",
       " 'furbishment',\n",
       " 'furnishing',\n",
       " 'graduate',\n",
       " 'imporvement',\n",
       " 'incluinde',\n",
       " 'liiiiittle',\n",
       " 'maintainance',\n",
       " 'maintainence',\n",
       " 'maintanance',\n",
       " 'maintance',\n",
       " 'maintenace',\n",
       " 'maintenance',\n",
       " 'maintence',\n",
       " 'maintenence',\n",
       " 'makeover',\n",
       " 'marriage',\n",
       " 'modernisation',\n",
       " 'modernise',\n",
       " 'modernising',\n",
       " 'modernization',\n",
       " 'modernize',\n",
       " 'modernized',\n",
       " 'nuptial',\n",
       " 'overhall',\n",
       " 'overhaul',\n",
       " 'overhauling',\n",
       " 'paylate',\n",
       " 'plumber',\n",
       " 'plumbing',\n",
       " 'proposal',\n",
       " 'reconstruction',\n",
       " 'redecorate',\n",
       " 'redecorating',\n",
       " 'redecoration',\n",
       " 'redeedme',\n",
       " 'redesign',\n",
       " 'redo',\n",
       " 'redone',\n",
       " 'referb',\n",
       " 'refinishing',\n",
       " 'refit',\n",
       " 'refresh',\n",
       " 'refurb',\n",
       " 'refurbe',\n",
       " 'refurbish',\n",
       " 'refurbished',\n",
       " 'refurbishing',\n",
       " 'refurbishment',\n",
       " 'refurnish',\n",
       " 'reglazing',\n",
       " 'relifting',\n",
       " 'remodel',\n",
       " 'remodelation',\n",
       " 'remodeling',\n",
       " 'remodelle',\n",
       " 'remodelling',\n",
       " 'remolding',\n",
       " 'renew',\n",
       " 'renewal',\n",
       " 'rennovate',\n",
       " 'rennovation',\n",
       " 'reno',\n",
       " 'renovate',\n",
       " 'renovated',\n",
       " 'renovating',\n",
       " 'renovation',\n",
       " 'repaint',\n",
       " 'repairing',\n",
       " 'replace',\n",
       " 'replacement',\n",
       " 'restoration',\n",
       " 'restore',\n",
       " 'resurface',\n",
       " 'resurfacing',\n",
       " 'retirement',\n",
       " 'revamp',\n",
       " 'revive',\n",
       " 'shabyin',\n",
       " 'sijanti',\n",
       " 'spruce',\n",
       " 'tlc',\n",
       " 'transformation',\n",
       " 'uodate',\n",
       " 'update',\n",
       " 'updateing',\n",
       " 'updating',\n",
       " 'upgrading',\n",
       " 'upgrading3',\n",
       " 'uplift',\n",
       " 'vow',\n",
       " 'weeper',\n",
       " 'yesare',\n",
       " '기다리세요라고',\n",
       " '되었습니다',\n",
       " '머물렀었는데',\n",
       " '먼저',\n",
       " '발레파킹은',\n",
       " '방문',\n",
       " '서서',\n",
       " '싶을',\n",
       " '없었습니다',\n",
       " '입구를',\n",
       " '정도였습니다',\n",
       " '주차공간을',\n",
       " '찾는데만',\n",
       " '하는데만']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(renos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yiyi/anaconda3/envs/cpd/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "keywords_similar = defaultdict(list)\n",
    "for words in keywords:\n",
    "    keyword= words[0]\n",
    "    for word in words:\n",
    "        try:\n",
    "            keywords_similar[keyword].append(list([x for x,y in model.most_similar(word, topn=20 )]))\n",
    "        except Exception:\n",
    "            pass\n",
    "    keywords_similar[keyword].append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yiyi/anaconda3/envs/cpd/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('noice', 0.8472913503646851),\n",
       " ('sound', 0.6614988446235657),\n",
       " ('noisy', 0.6463830471038818),\n",
       " ('banging', 0.5855908393859863),\n",
       " ('ruckus', 0.5773318409919739),\n",
       " ('audible', 0.5717959403991699),\n",
       " ('loud', 0.5664687156677246),\n",
       " ('rumble', 0.5656085014343262),\n",
       " ('noisey', 0.5468951463699341),\n",
       " ('jackhammering', 0.5466371178627014),\n",
       " ('commotion', 0.5412279367446899),\n",
       " ('hum', 0.5396388173103333),\n",
       " ('disturbance', 0.5368995666503906),\n",
       " ('thumping', 0.533816933631897),\n",
       " ('disruptive', 0.5334421396255493),\n",
       " ('thud', 0.5293156504631042),\n",
       " ('siren', 0.5273467302322388),\n",
       " ('pounding', 0.5225614905357361),\n",
       " ('humming', 0.518813967704773),\n",
       " ('drilling', 0.5138258934020996)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"noise\", topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "keywords_similars = {}\n",
    "for keyword, value in keywords_similar.items():\n",
    "    keywords_similars[keyword]=sorted(list(set(list(chain.from_iterable(value))))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('keywords.json', 'w') as file:\n",
    "    json.dump(keywords_similars, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('keywords.json', 'w')as file:\n",
    "    json.dump(keywords_similar, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspects = []\n",
    "for key, value in keywords_similar.items():\n",
    "    aspects.append(key)\n",
    "    aspects+= value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(set(aspects)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yiyi/anaconda3/envs/cpd/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "keywords_reno = {}\n",
    "for word in reno_keywords:\n",
    "    try:\n",
    "        keywords_reno[word]=list([x for x,y in model.most_similar(word, topn=20 )])\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "renowords = []\n",
    "for key, value in keywords_reno.items():\n",
    "    renowords.append(key)\n",
    "    renowords+= value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted(list(set(renowords)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpd",
   "language": "python",
   "name": "cdp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
