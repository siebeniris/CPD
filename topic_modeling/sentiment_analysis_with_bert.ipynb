{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('DATASET/Hotel_Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hotel_Address</th>\n",
       "      <th>Additional_Number_of_Scoring</th>\n",
       "      <th>Review_Date</th>\n",
       "      <th>Average_Score</th>\n",
       "      <th>Hotel_Name</th>\n",
       "      <th>Reviewer_Nationality</th>\n",
       "      <th>Negative_Review</th>\n",
       "      <th>Review_Total_Negative_Word_Counts</th>\n",
       "      <th>Total_Number_of_Reviews</th>\n",
       "      <th>Positive_Review</th>\n",
       "      <th>Review_Total_Positive_Word_Counts</th>\n",
       "      <th>Total_Number_of_Reviews_Reviewer_Has_Given</th>\n",
       "      <th>Reviewer_Score</th>\n",
       "      <th>Tags</th>\n",
       "      <th>days_since_review</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s Gravesandestraat 55 Oost 1092 AA Amsterdam ...</td>\n",
       "      <td>194</td>\n",
       "      <td>8/3/2017</td>\n",
       "      <td>7.7</td>\n",
       "      <td>Hotel Arena</td>\n",
       "      <td>Russia</td>\n",
       "      <td>I am so angry that i made this post available...</td>\n",
       "      <td>397</td>\n",
       "      <td>1403</td>\n",
       "      <td>Only the park outside of the hotel was beauti...</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>2.9</td>\n",
       "      <td>[' Leisure trip ', ' Couple ', ' Duplex Double...</td>\n",
       "      <td>0 days</td>\n",
       "      <td>52.360576</td>\n",
       "      <td>4.915968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s Gravesandestraat 55 Oost 1092 AA Amsterdam ...</td>\n",
       "      <td>194</td>\n",
       "      <td>8/3/2017</td>\n",
       "      <td>7.7</td>\n",
       "      <td>Hotel Arena</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>No Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>1403</td>\n",
       "      <td>No real complaints the hotel was great great ...</td>\n",
       "      <td>105</td>\n",
       "      <td>7</td>\n",
       "      <td>7.5</td>\n",
       "      <td>[' Leisure trip ', ' Couple ', ' Duplex Double...</td>\n",
       "      <td>0 days</td>\n",
       "      <td>52.360576</td>\n",
       "      <td>4.915968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s Gravesandestraat 55 Oost 1092 AA Amsterdam ...</td>\n",
       "      <td>194</td>\n",
       "      <td>7/31/2017</td>\n",
       "      <td>7.7</td>\n",
       "      <td>Hotel Arena</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Rooms are nice but for elderly a bit difficul...</td>\n",
       "      <td>42</td>\n",
       "      <td>1403</td>\n",
       "      <td>Location was good and staff were ok It is cut...</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>7.1</td>\n",
       "      <td>[' Leisure trip ', ' Family with young childre...</td>\n",
       "      <td>3 days</td>\n",
       "      <td>52.360576</td>\n",
       "      <td>4.915968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s Gravesandestraat 55 Oost 1092 AA Amsterdam ...</td>\n",
       "      <td>194</td>\n",
       "      <td>7/31/2017</td>\n",
       "      <td>7.7</td>\n",
       "      <td>Hotel Arena</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>My room was dirty and I was afraid to walk ba...</td>\n",
       "      <td>210</td>\n",
       "      <td>1403</td>\n",
       "      <td>Great location in nice surroundings the bar a...</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>[' Leisure trip ', ' Solo traveler ', ' Duplex...</td>\n",
       "      <td>3 days</td>\n",
       "      <td>52.360576</td>\n",
       "      <td>4.915968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s Gravesandestraat 55 Oost 1092 AA Amsterdam ...</td>\n",
       "      <td>194</td>\n",
       "      <td>7/24/2017</td>\n",
       "      <td>7.7</td>\n",
       "      <td>Hotel Arena</td>\n",
       "      <td>New Zealand</td>\n",
       "      <td>You When I booked with your company on line y...</td>\n",
       "      <td>140</td>\n",
       "      <td>1403</td>\n",
       "      <td>Amazing location and building Romantic setting</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>6.7</td>\n",
       "      <td>[' Leisure trip ', ' Couple ', ' Suite ', ' St...</td>\n",
       "      <td>10 days</td>\n",
       "      <td>52.360576</td>\n",
       "      <td>4.915968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Hotel_Address  \\\n",
       "0   s Gravesandestraat 55 Oost 1092 AA Amsterdam ...   \n",
       "1   s Gravesandestraat 55 Oost 1092 AA Amsterdam ...   \n",
       "2   s Gravesandestraat 55 Oost 1092 AA Amsterdam ...   \n",
       "3   s Gravesandestraat 55 Oost 1092 AA Amsterdam ...   \n",
       "4   s Gravesandestraat 55 Oost 1092 AA Amsterdam ...   \n",
       "\n",
       "   Additional_Number_of_Scoring Review_Date  Average_Score   Hotel_Name  \\\n",
       "0                           194    8/3/2017            7.7  Hotel Arena   \n",
       "1                           194    8/3/2017            7.7  Hotel Arena   \n",
       "2                           194   7/31/2017            7.7  Hotel Arena   \n",
       "3                           194   7/31/2017            7.7  Hotel Arena   \n",
       "4                           194   7/24/2017            7.7  Hotel Arena   \n",
       "\n",
       "  Reviewer_Nationality                                    Negative_Review  \\\n",
       "0              Russia    I am so angry that i made this post available...   \n",
       "1             Ireland                                         No Negative   \n",
       "2           Australia    Rooms are nice but for elderly a bit difficul...   \n",
       "3      United Kingdom    My room was dirty and I was afraid to walk ba...   \n",
       "4         New Zealand    You When I booked with your company on line y...   \n",
       "\n",
       "   Review_Total_Negative_Word_Counts  Total_Number_of_Reviews  \\\n",
       "0                                397                     1403   \n",
       "1                                  0                     1403   \n",
       "2                                 42                     1403   \n",
       "3                                210                     1403   \n",
       "4                                140                     1403   \n",
       "\n",
       "                                     Positive_Review  \\\n",
       "0   Only the park outside of the hotel was beauti...   \n",
       "1   No real complaints the hotel was great great ...   \n",
       "2   Location was good and staff were ok It is cut...   \n",
       "3   Great location in nice surroundings the bar a...   \n",
       "4    Amazing location and building Romantic setting    \n",
       "\n",
       "   Review_Total_Positive_Word_Counts  \\\n",
       "0                                 11   \n",
       "1                                105   \n",
       "2                                 21   \n",
       "3                                 26   \n",
       "4                                  8   \n",
       "\n",
       "   Total_Number_of_Reviews_Reviewer_Has_Given  Reviewer_Score  \\\n",
       "0                                           7             2.9   \n",
       "1                                           7             7.5   \n",
       "2                                           9             7.1   \n",
       "3                                           1             3.8   \n",
       "4                                           3             6.7   \n",
       "\n",
       "                                                Tags days_since_review  \\\n",
       "0  [' Leisure trip ', ' Couple ', ' Duplex Double...            0 days   \n",
       "1  [' Leisure trip ', ' Couple ', ' Duplex Double...            0 days   \n",
       "2  [' Leisure trip ', ' Family with young childre...            3 days   \n",
       "3  [' Leisure trip ', ' Solo traveler ', ' Duplex...            3 days   \n",
       "4  [' Leisure trip ', ' Couple ', ' Suite ', ' St...           10 days   \n",
       "\n",
       "         lat       lng  \n",
       "0  52.360576  4.915968  \n",
       "1  52.360576  4.915968  \n",
       "2  52.360576  4.915968  \n",
       "3  52.360576  4.915968  \n",
       "4  52.360576  4.915968  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_reviews = [x for x in reviews['Positive_Review'] if x!='No Positive']\n",
    "neg_reviews = [x for x in reviews['Negative_Review'] if x!='No Negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(479792, 387848, 515738)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_reviews), len(neg_reviews), len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_words_lens = max(reviews['Review_Total_Negative_Word_Counts'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "395"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(reviews['Review_Total_Positive_Word_Counts'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "408"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_words_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load all files from a directory in a DataFrame.\n",
    "def load_directory_data(directory):\n",
    "    data = {}\n",
    "    data[\"sentence\"] = []\n",
    "    data[\"sentiment\"] = []\n",
    "    for file_path in os.listdir(directory):\n",
    "        with tf.io.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
    "            data[\"sentence\"].append(f.read())\n",
    "            data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "    return pd.DataFrame.from_dict(data)\n",
    "\n",
    "# Merge positive and negative examples, add a polarity column and shuffle.\n",
    "def load_dataset(directory):\n",
    "    pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
    "    neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
    "    pos_df[\"polarity\"] = 1\n",
    "    neg_df[\"polarity\"] = 0\n",
    "    return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Download and process the dataset files.\n",
    "def download_and_load_datasets(force_download=False):\n",
    "    dataset = tf.keras.utils.get_file(\n",
    "      fname=\"aclImdb.tar.gz\", \n",
    "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
    "      extract=True)\n",
    "\n",
    "    train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                       \"aclImdb\", \"train\"))\n",
    "    test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                      \"aclImdb\", \"test\"))\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = download_and_load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sample(5000)\n",
    "test = test.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3534</th>\n",
       "      <td>I've only watched the first series on DVD, but...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18013</th>\n",
       "      <td>Having lived in Japan for several years this m...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17621</th>\n",
       "      <td>Bellocchio refers to this as a mainly politica...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>I can find very little thats good to say about...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10240</th>\n",
       "      <td>I remember Devil Dog playing on TBS almost 20 ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence sentiment  polarity\n",
       "3534   I've only watched the first series on DVD, but...         9         1\n",
       "18013  Having lived in Japan for several years this m...         1         0\n",
       "17621  Bellocchio refers to this as a mainly politica...         7         1\n",
       "862    I can find very little thats good to say about...         1         0\n",
       "10240  I remember Devil Dog playing on TBS almost 20 ...         4         0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, text_a = x['sentence'], text_b = None, label = x['polarity']), axis = 1)\n",
    "\n",
    "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, text_a = x['sentence'], text_b = None,label = x['polarity']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3534     <bert.run_classifier.InputExample object at 0x...\n",
       "18013    <bert.run_classifier.InputExample object at 0x...\n",
       "17621    <bert.run_classifier.InputExample object at 0x...\n",
       "862      <bert.run_classifier.InputExample object at 0x...\n",
       "10240    <bert.run_classifier.InputExample object at 0x...\n",
       "                               ...                        \n",
       "13556    <bert.run_classifier.InputExample object at 0x...\n",
       "7541     <bert.run_classifier.InputExample object at 0x...\n",
       "11077    <bert.run_classifier.InputExample object at 0x...\n",
       "22928    <bert.run_classifier.InputExample object at 0x...\n",
       "21356    <bert.run_classifier.InputExample object at 0x...\n",
       "Length: 5000, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_InputExamples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\n",
    "\n",
    "- Lowercase our text (if we're using a BERT lowercase model)\n",
    "- Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
    "- Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
    "- Map our words to indexes using a vocab file that BERT provides\n",
    "- Add special \"CLS\" and \"SEP\" tokens (see the readme)\n",
    "- Append \"index\" and \"segment\" tokens to each input (see the BERT paper)\n",
    "Happily, we don't have to worry about most of these details.\n",
    "\n",
    "To start, we'll need to load a vocabulary file and lowercasing information directly from the BERT tf hub module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "# This is a path to an uncased (all lowercase) version of BERT\n",
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "        with tf.compat.v1.Session() as sess:\n",
    "              vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                                    tokenization_info[\"do_lower_case\"]])\n",
    "\n",
    "    return bert.tokenization.FullTokenizer(\n",
    "          vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'here',\n",
       " \"'\",\n",
       " 's',\n",
       " 'an',\n",
       " 'example',\n",
       " 'of',\n",
       " 'using',\n",
       " 'the',\n",
       " 'bert',\n",
       " 'token',\n",
       " '##izer']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] i ' ve only watched the first series on dvd , but would sum ##mar ##ise the soprano ##s as a shakespeare ##an plot with a tara ##ntino - like script . the series is as good as good ##fell ##as and casino , and almost as good as the godfather ( hence not a \" 10 \" ) , and far better than any of guy ritchie ' s efforts . although there ' s plenty of action , some of it pretty bloody , the story is character driven . even some of the minor characters contribute to great story lines ; e . g . the priest ' s relationship ( or lack of ) with car ##mill ##a and the rest ##aur [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] i ' ve only watched the first series on dvd , but would sum ##mar ##ise the soprano ##s as a shakespeare ##an plot with a tara ##ntino - like script . the series is as good as good ##fell ##as and casino , and almost as good as the godfather ( hence not a \" 10 \" ) , and far better than any of guy ritchie ' s efforts . although there ' s plenty of action , some of it pretty bloody , the story is character driven . even some of the minor characters contribute to great story lines ; e . g . the priest ' s relationship ( or lack of ) with car ##mill ##a and the rest ##aur [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1045 1005 2310 2069 3427 1996 2034 2186 2006 4966 1010 2021 2052 7680 7849 5562 1996 10430 2015 2004 1037 8101 2319 5436 2007 1037 10225 25318 1011 2066 5896 1012 1996 2186 2003 2004 2204 2004 2204 23510 3022 1998 9270 1010 1998 2471 2004 2204 2004 1996 23834 1006 6516 2025 1037 1000 2184 1000 1007 1010 1998 2521 2488 2084 2151 1997 3124 20404 1005 1055 4073 1012 2348 2045 1005 1055 7564 1997 2895 1010 2070 1997 2009 3492 6703 1010 1996 2466 2003 2839 5533 1012 2130 2070 1997 1996 3576 3494 9002 2000 2307 2466 3210 1025 1041 1012 1043 1012 1996 5011 1005 1055 3276 1006 2030 3768 1997 1007 2007 2482 19912 2050 1998 1996 2717 21159 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1045 1005 2310 2069 3427 1996 2034 2186 2006 4966 1010 2021 2052 7680 7849 5562 1996 10430 2015 2004 1037 8101 2319 5436 2007 1037 10225 25318 1011 2066 5896 1012 1996 2186 2003 2004 2204 2004 2204 23510 3022 1998 9270 1010 1998 2471 2004 2204 2004 1996 23834 1006 6516 2025 1037 1000 2184 1000 1007 1010 1998 2521 2488 2084 2151 1997 3124 20404 1005 1055 4073 1012 2348 2045 1005 1055 7564 1997 2895 1010 2070 1997 2009 3492 6703 1010 1996 2466 2003 2839 5533 1012 2130 2070 1997 1996 3576 3494 9002 2000 2307 2466 3210 1025 1041 1012 1043 1012 1996 5011 1005 1055 3276 1006 2030 3768 1997 1007 2007 2482 19912 2050 1998 1996 2717 21159 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] having lived in japan for several years this movie does not reflect the japanese culture and does not even come close to explain what being a ge ##isha is all about . unfortunately , a great opportunity has been missed to bring the japanese culture a bit closer to the broad western audience and help dem ##yst ##ify the country where zen , samurai , the ge ##isha world of kyoto originate from . some of the most po ##ignant moments of the movie are when the americans are shown in japanese surroundings . the ge ##isha dances were not authentic . there was far too much use of chinese music . a minor but essential detail : proper use of the incense sticks was nowhere [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] having lived in japan for several years this movie does not reflect the japanese culture and does not even come close to explain what being a ge ##isha is all about . unfortunately , a great opportunity has been missed to bring the japanese culture a bit closer to the broad western audience and help dem ##yst ##ify the country where zen , samurai , the ge ##isha world of kyoto originate from . some of the most po ##ignant moments of the movie are when the americans are shown in japanese surroundings . the ge ##isha dances were not authentic . there was far too much use of chinese music . a minor but essential detail : proper use of the incense sticks was nowhere [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2383 2973 1999 2900 2005 2195 2086 2023 3185 2515 2025 8339 1996 2887 3226 1998 2515 2025 2130 2272 2485 2000 4863 2054 2108 1037 16216 24032 2003 2035 2055 1012 6854 1010 1037 2307 4495 2038 2042 4771 2000 3288 1996 2887 3226 1037 2978 3553 2000 1996 5041 2530 4378 1998 2393 17183 27268 8757 1996 2406 2073 16729 1010 16352 1010 1996 16216 24032 2088 1997 15008 21754 2013 1012 2070 1997 1996 2087 13433 25593 5312 1997 1996 3185 2024 2043 1996 4841 2024 3491 1999 2887 11301 1012 1996 16216 24032 11278 2020 2025 14469 1012 2045 2001 2521 2205 2172 2224 1997 2822 2189 1012 1037 3576 2021 6827 6987 1024 5372 2224 1997 1996 28647 12668 2001 7880 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2383 2973 1999 2900 2005 2195 2086 2023 3185 2515 2025 8339 1996 2887 3226 1998 2515 2025 2130 2272 2485 2000 4863 2054 2108 1037 16216 24032 2003 2035 2055 1012 6854 1010 1037 2307 4495 2038 2042 4771 2000 3288 1996 2887 3226 1037 2978 3553 2000 1996 5041 2530 4378 1998 2393 17183 27268 8757 1996 2406 2073 16729 1010 16352 1010 1996 16216 24032 2088 1997 15008 21754 2013 1012 2070 1997 1996 2087 13433 25593 5312 1997 1996 3185 2024 2043 1996 4841 2024 3491 1999 2887 11301 1012 1996 16216 24032 11278 2020 2025 14469 1012 2045 2001 2521 2205 2172 2224 1997 2822 2189 1012 1037 3576 2021 6827 6987 1024 5372 2224 1997 1996 28647 12668 2001 7880 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] bell ##oc ##chio refers to this as a mainly political movie , a description of the revolutionary movement in italy , but that seems more metaphor than reality . well , almost everything in the movie seems like metaphor . the revolutionaries , of whom we see and about whom we learn very little , might as well be ma ##fi ##osi . out with the old and in with the new . < br / > < br / > andrea ' s papa , a psycho ##anal ##yst , seems to stand for the usual traditional bourgeois values - - morally upright , un ##per ##tur ##bed , clean and tidy , thoroughly ritual ##ized . < br / > < br / > [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] bell ##oc ##chio refers to this as a mainly political movie , a description of the revolutionary movement in italy , but that seems more metaphor than reality . well , almost everything in the movie seems like metaphor . the revolutionaries , of whom we see and about whom we learn very little , might as well be ma ##fi ##osi . out with the old and in with the new . < br / > < br / > andrea ' s papa , a psycho ##anal ##yst , seems to stand for the usual traditional bourgeois values - - morally upright , un ##per ##tur ##bed , clean and tidy , thoroughly ritual ##ized . < br / > < br / > [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 4330 10085 23584 5218 2000 2023 2004 1037 3701 2576 3185 1010 1037 6412 1997 1996 6208 2929 1999 3304 1010 2021 2008 3849 2062 19240 2084 4507 1012 2092 1010 2471 2673 1999 1996 3185 3849 2066 19240 1012 1996 24517 1010 1997 3183 2057 2156 1998 2055 3183 2057 4553 2200 2210 1010 2453 2004 2092 2022 5003 8873 20049 1012 2041 2007 1996 2214 1998 1999 2007 1996 2047 1012 1026 7987 1013 1028 1026 7987 1013 1028 8657 1005 1055 13008 1010 1037 18224 27953 27268 1010 3849 2000 3233 2005 1996 5156 3151 22846 5300 1011 1011 28980 10051 1010 4895 4842 20689 8270 1010 4550 1998 29369 1010 12246 8887 3550 1012 1026 7987 1013 1028 1026 7987 1013 1028 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 4330 10085 23584 5218 2000 2023 2004 1037 3701 2576 3185 1010 1037 6412 1997 1996 6208 2929 1999 3304 1010 2021 2008 3849 2062 19240 2084 4507 1012 2092 1010 2471 2673 1999 1996 3185 3849 2066 19240 1012 1996 24517 1010 1997 3183 2057 2156 1998 2055 3183 2057 4553 2200 2210 1010 2453 2004 2092 2022 5003 8873 20049 1012 2041 2007 1996 2214 1998 1999 2007 1996 2047 1012 1026 7987 1013 1028 1026 7987 1013 1028 8657 1005 1055 13008 1010 1037 18224 27953 27268 1010 3849 2000 3233 2005 1996 5156 3151 22846 5300 1011 1011 28980 10051 1010 4895 4842 20689 8270 1010 4550 1998 29369 1010 12246 8887 3550 1012 1026 7987 1013 1028 1026 7987 1013 1028 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] i can find very little that ##s good to say about this film . i am sure the idea and script looked good on paper but the film ##ography and acting i am afraid is not the standards i would expect from some very talented people . i would doubt that this features highly in their cv film ##ography . michael caine appeared wooden at times in his role as the doctor , and at no time no did i actually believe in his character . the plot was unbelievable especially with regard to the victims son . some of the scenes were very reminiscent of other films , that at times i wondered if it was actually a sp ##oof thriller . the lighting at [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] i can find very little that ##s good to say about this film . i am sure the idea and script looked good on paper but the film ##ography and acting i am afraid is not the standards i would expect from some very talented people . i would doubt that this features highly in their cv film ##ography . michael caine appeared wooden at times in his role as the doctor , and at no time no did i actually believe in his character . the plot was unbelievable especially with regard to the victims son . some of the scenes were very reminiscent of other films , that at times i wondered if it was actually a sp ##oof thriller . the lighting at [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1045 2064 2424 2200 2210 2008 2015 2204 2000 2360 2055 2023 2143 1012 1045 2572 2469 1996 2801 1998 5896 2246 2204 2006 3259 2021 1996 2143 9888 1998 3772 1045 2572 4452 2003 2025 1996 4781 1045 2052 5987 2013 2070 2200 10904 2111 1012 1045 2052 4797 2008 2023 2838 3811 1999 2037 26226 2143 9888 1012 2745 19881 2596 4799 2012 2335 1999 2010 2535 2004 1996 3460 1010 1998 2012 2053 2051 2053 2106 1045 2941 2903 1999 2010 2839 1012 1996 5436 2001 23653 2926 2007 7634 2000 1996 5694 2365 1012 2070 1997 1996 5019 2020 2200 14563 1997 2060 3152 1010 2008 2012 2335 1045 4999 2065 2009 2001 2941 1037 11867 21511 10874 1012 1996 7497 2012 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1045 2064 2424 2200 2210 2008 2015 2204 2000 2360 2055 2023 2143 1012 1045 2572 2469 1996 2801 1998 5896 2246 2204 2006 3259 2021 1996 2143 9888 1998 3772 1045 2572 4452 2003 2025 1996 4781 1045 2052 5987 2013 2070 2200 10904 2111 1012 1045 2052 4797 2008 2023 2838 3811 1999 2037 26226 2143 9888 1012 2745 19881 2596 4799 2012 2335 1999 2010 2535 2004 1996 3460 1010 1998 2012 2053 2051 2053 2106 1045 2941 2903 1999 2010 2839 1012 1996 5436 2001 23653 2926 2007 7634 2000 1996 5694 2365 1012 2070 1997 1996 5019 2020 2200 14563 1997 2060 3152 1010 2008 2012 2335 1045 4999 2065 2009 2001 2941 1037 11867 21511 10874 1012 1996 7497 2012 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] i remember devil dog playing on tbs almost 20 years ago , and my older sister and her friends watching it and laughing all the next day . it ' s not that bad for a made - for - tv horror movie , but it is derivative ( mostly of the ex ##or ##cis ##t ) and business ##like , for lack of a better word . it won ' t blow you away with art ##ful cinematography or great acting , but it ' s not a waste of time , either . it ' s the kind of movie you watch to kill a couple of hours when you aren ' t in the mood to think too hard . < br / [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] i remember devil dog playing on tbs almost 20 years ago , and my older sister and her friends watching it and laughing all the next day . it ' s not that bad for a made - for - tv horror movie , but it is derivative ( mostly of the ex ##or ##cis ##t ) and business ##like , for lack of a better word . it won ' t blow you away with art ##ful cinematography or great acting , but it ' s not a waste of time , either . it ' s the kind of movie you watch to kill a couple of hours when you aren ' t in the mood to think too hard . < br / [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1045 3342 6548 3899 2652 2006 29584 2471 2322 2086 3283 1010 1998 2026 3080 2905 1998 2014 2814 3666 2009 1998 5870 2035 1996 2279 2154 1012 2009 1005 1055 2025 2008 2919 2005 1037 2081 1011 2005 1011 2694 5469 3185 1010 2021 2009 2003 13819 1006 3262 1997 1996 4654 2953 22987 2102 1007 1998 2449 10359 1010 2005 3768 1997 1037 2488 2773 1012 2009 2180 1005 1056 6271 2017 2185 2007 2396 3993 16434 2030 2307 3772 1010 2021 2009 1005 1055 2025 1037 5949 1997 2051 1010 2593 1012 2009 1005 1055 1996 2785 1997 3185 2017 3422 2000 3102 1037 3232 1997 2847 2043 2017 4995 1005 1056 1999 1996 6888 2000 2228 2205 2524 1012 1026 7987 1013 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1045 3342 6548 3899 2652 2006 29584 2471 2322 2086 3283 1010 1998 2026 3080 2905 1998 2014 2814 3666 2009 1998 5870 2035 1996 2279 2154 1012 2009 1005 1055 2025 2008 2919 2005 1037 2081 1011 2005 1011 2694 5469 3185 1010 2021 2009 2003 13819 1006 3262 1997 1996 4654 2953 22987 2102 1007 1998 2449 10359 1010 2005 3768 1997 1037 2488 2773 1012 2009 2180 1005 1056 6271 2017 2185 2007 2396 3993 16434 2030 2307 3772 1010 2021 2009 1005 1055 2025 1037 5949 1997 2051 1010 2593 1012 2009 1005 1055 1996 2785 1997 3185 2017 3422 2000 3102 1037 3232 1997 2847 2043 2017 4995 1005 1056 1999 1996 6888 2000 2228 2205 2524 1012 1026 7987 1013 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] there is a clever little scene in the karen carpenter story , where both carpenter ##s are in a recording studio , and richard makes an impromptu decision to have karen sing for the owner of the studio . < br / > < br / > richard picks the wrong key for karen to sing in , so karen is singing above her natural range . you can see a look of be ##mus ##ement on the owner ' s face ; he figures she really can ' t sing . richard quickly realizes his mistake and tries again in a different key . the next thing you hear is karen ' s amazing , beautiful voice , and the owner does a price ##less [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] there is a clever little scene in the karen carpenter story , where both carpenter ##s are in a recording studio , and richard makes an impromptu decision to have karen sing for the owner of the studio . < br / > < br / > richard picks the wrong key for karen to sing in , so karen is singing above her natural range . you can see a look of be ##mus ##ement on the owner ' s face ; he figures she really can ' t sing . richard quickly realizes his mistake and tries again in a different key . the next thing you hear is karen ' s amazing , beautiful voice , and the owner does a price ##less [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2045 2003 1037 12266 2210 3496 1999 1996 8129 10533 2466 1010 2073 2119 10533 2015 2024 1999 1037 3405 2996 1010 1998 2957 3084 2019 29213 3247 2000 2031 8129 6170 2005 1996 3954 1997 1996 2996 1012 1026 7987 1013 1028 1026 7987 1013 1028 2957 11214 1996 3308 3145 2005 8129 2000 6170 1999 1010 2061 8129 2003 4823 2682 2014 3019 2846 1012 2017 2064 2156 1037 2298 1997 2022 7606 13665 2006 1996 3954 1005 1055 2227 1025 2002 4481 2016 2428 2064 1005 1056 6170 1012 2957 2855 10919 2010 6707 1998 5363 2153 1999 1037 2367 3145 1012 1996 2279 2518 2017 2963 2003 8129 1005 1055 6429 1010 3376 2376 1010 1998 1996 3954 2515 1037 3976 3238 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2045 2003 1037 12266 2210 3496 1999 1996 8129 10533 2466 1010 2073 2119 10533 2015 2024 1999 1037 3405 2996 1010 1998 2957 3084 2019 29213 3247 2000 2031 8129 6170 2005 1996 3954 1997 1996 2996 1012 1026 7987 1013 1028 1026 7987 1013 1028 2957 11214 1996 3308 3145 2005 8129 2000 6170 1999 1010 2061 8129 2003 4823 2682 2014 3019 2846 1012 2017 2064 2156 1037 2298 1997 2022 7606 13665 2006 1996 3954 1005 1055 2227 1025 2002 4481 2016 2428 2064 1005 1056 6170 1012 2957 2855 10919 2010 6707 1998 5363 2153 1999 1037 2367 3145 1012 1996 2279 2518 2017 2963 2003 8129 1005 1055 6429 1010 3376 2376 1010 1998 1996 3954 2515 1037 3976 3238 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] after seeing the trailer it was an easy decision not to see this film . i mean , i don ' t care for stupid \" stone ##r comedies . \" i ' m sure it was also an easy choice for a lot of people to get together , smoke a bowl and go check out this flick with the guy from the simpsons and some guy named \" billy bob . \" should have been a good time , but the film ' s just not that funny - - too bad somebody had to go and bum their high . < br / > < br / > unfortunately , i found out that the trailer was misleading after it had already left [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] after seeing the trailer it was an easy decision not to see this film . i mean , i don ' t care for stupid \" stone ##r comedies . \" i ' m sure it was also an easy choice for a lot of people to get together , smoke a bowl and go check out this flick with the guy from the simpsons and some guy named \" billy bob . \" should have been a good time , but the film ' s just not that funny - - too bad somebody had to go and bum their high . < br / > < br / > unfortunately , i found out that the trailer was misleading after it had already left [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2044 3773 1996 9117 2009 2001 2019 3733 3247 2025 2000 2156 2023 2143 1012 1045 2812 1010 1045 2123 1005 1056 2729 2005 5236 1000 2962 2099 22092 1012 1000 1045 1005 1049 2469 2009 2001 2036 2019 3733 3601 2005 1037 2843 1997 2111 2000 2131 2362 1010 5610 1037 4605 1998 2175 4638 2041 2023 17312 2007 1996 3124 2013 1996 19047 1998 2070 3124 2315 1000 5006 3960 1012 1000 2323 2031 2042 1037 2204 2051 1010 2021 1996 2143 1005 1055 2074 2025 2008 6057 1011 1011 2205 2919 8307 2018 2000 2175 1998 26352 2037 2152 1012 1026 7987 1013 1028 1026 7987 1013 1028 6854 1010 1045 2179 2041 2008 1996 9117 2001 22369 2044 2009 2018 2525 2187 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2044 3773 1996 9117 2009 2001 2019 3733 3247 2025 2000 2156 2023 2143 1012 1045 2812 1010 1045 2123 1005 1056 2729 2005 5236 1000 2962 2099 22092 1012 1000 1045 1005 1049 2469 2009 2001 2036 2019 3733 3601 2005 1037 2843 1997 2111 2000 2131 2362 1010 5610 1037 4605 1998 2175 4638 2041 2023 17312 2007 1996 3124 2013 1996 19047 1998 2070 3124 2315 1000 5006 3960 1012 1000 2323 2031 2042 1037 2204 2051 1010 2021 1996 2143 1005 1055 2074 2025 2008 6057 1011 1011 2205 2919 8307 2018 2000 2175 1998 26352 2037 2152 1012 1026 7987 1013 1028 1026 7987 1013 1028 6854 1010 1045 2179 2041 2008 1996 9117 2001 22369 2044 2009 2018 2525 2187 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] niagara , niagara is a stunning and heartbreak ##ing story about the two outsiders seth and marc ##y . robin tun ##ney gives a fantastic performance as marc ##y suffering from tour ##ette ' s sy ##drome , getting sick ##er and sick ##er as the movie progresses . this movie is not very optimistic and it ' s very hard emotional , but at the same time very romantic . it ' s hard to explain , but see it and find out for yourself . it ' s definitely worth it . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] niagara , niagara is a stunning and heartbreak ##ing story about the two outsiders seth and marc ##y . robin tun ##ney gives a fantastic performance as marc ##y suffering from tour ##ette ' s sy ##drome , getting sick ##er and sick ##er as the movie progresses . this movie is not very optimistic and it ' s very hard emotional , but at the same time very romantic . it ' s hard to explain , but see it and find out for yourself . it ' s definitely worth it . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 15473 1010 15473 2003 1037 14726 1998 27724 2075 2466 2055 1996 2048 22361 6662 1998 7871 2100 1012 5863 27112 5420 3957 1037 10392 2836 2004 7871 2100 6114 2013 2778 7585 1005 1055 25353 29171 1010 2893 5305 2121 1998 5305 2121 2004 1996 3185 22901 1012 2023 3185 2003 2025 2200 21931 1998 2009 1005 1055 2200 2524 6832 1010 2021 2012 1996 2168 2051 2200 6298 1012 2009 1005 1055 2524 2000 4863 1010 2021 2156 2009 1998 2424 2041 2005 4426 1012 2009 1005 1055 5791 4276 2009 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 15473 1010 15473 2003 1037 14726 1998 27724 2075 2466 2055 1996 2048 22361 6662 1998 7871 2100 1012 5863 27112 5420 3957 1037 10392 2836 2004 7871 2100 6114 2013 2778 7585 1005 1055 25353 29171 1010 2893 5305 2121 1998 5305 2121 2004 1996 3185 22901 1012 2023 3185 2003 2025 2200 21931 1998 2009 1005 1055 2200 2524 6832 1010 2021 2012 1996 2168 2051 2200 6298 1012 2009 1005 1055 2524 2000 4863 1010 2021 2156 2009 1998 2424 2041 2005 4426 1012 2009 1005 1055 5791 4276 2009 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] garden state must rate amongst the most con ##tri ##ved and pre ##ten ##tious films of all time . the plot is a simple one , involving a young man returning home after his mother ' s death and discovering love . but really , the plot isn ' t important . what is important to zach bra ##ff - writer , director , and star is that he is able to hang from the plot all the necessary acc ##out ##rem ##ents of an ' indie ' or ' art ##y ' film . we therefore are presented with endless cute and qui ##rky characters and scenes that don ' t exist for reasons of plot or character development , but simply to give some [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] garden state must rate amongst the most con ##tri ##ved and pre ##ten ##tious films of all time . the plot is a simple one , involving a young man returning home after his mother ' s death and discovering love . but really , the plot isn ' t important . what is important to zach bra ##ff - writer , director , and star is that he is able to hang from the plot all the necessary acc ##out ##rem ##ents of an ' indie ' or ' art ##y ' film . we therefore are presented with endless cute and qui ##rky characters and scenes that don ' t exist for reasons of plot or character development , but simply to give some [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 3871 2110 2442 3446 5921 1996 2087 9530 18886 7178 1998 3653 6528 20771 3152 1997 2035 2051 1012 1996 5436 2003 1037 3722 2028 1010 5994 1037 2402 2158 4192 2188 2044 2010 2388 1005 1055 2331 1998 13648 2293 1012 2021 2428 1010 1996 5436 3475 1005 1056 2590 1012 2054 2003 2590 2000 12397 11655 4246 1011 3213 1010 2472 1010 1998 2732 2003 2008 2002 2003 2583 2000 6865 2013 1996 5436 2035 1996 4072 16222 5833 28578 11187 1997 2019 1005 10271 1005 2030 1005 2396 2100 1005 2143 1012 2057 3568 2024 3591 2007 10866 10140 1998 21864 15952 3494 1998 5019 2008 2123 1005 1056 4839 2005 4436 1997 5436 2030 2839 2458 1010 2021 3432 2000 2507 2070 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 3871 2110 2442 3446 5921 1996 2087 9530 18886 7178 1998 3653 6528 20771 3152 1997 2035 2051 1012 1996 5436 2003 1037 3722 2028 1010 5994 1037 2402 2158 4192 2188 2044 2010 2388 1005 1055 2331 1998 13648 2293 1012 2021 2428 1010 1996 5436 3475 1005 1056 2590 1012 2054 2003 2590 2000 12397 11655 4246 1011 3213 1010 2472 1010 1998 2732 2003 2008 2002 2003 2583 2000 6865 2013 1996 5436 2035 1996 4072 16222 5833 28578 11187 1997 2019 1005 10271 1005 2030 1005 2396 2100 1005 2143 1012 2057 3568 2024 3591 2007 10866 10140 1998 21864 15952 3494 1998 5019 2008 2123 1005 1056 4839 2005 4436 1997 5436 2030 2839 2458 1010 2021 3432 2000 2507 2070 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] i was looking forward to this flick . being an old robert e howard fan , mainly from a conan stand - point . < br / > < br / > i was not expecting a great deal and thought they could not mess it up too much . . . . oh dear - how wrong was i . . . . < br / > < br / > the main flaw was it was fairly dull . it needed to zip along with a nice helping of supernatural going ##s - on , sword - fights and the like . < br / > < br / > you got some gore , but everything else was just pretty life - less [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] i was looking forward to this flick . being an old robert e howard fan , mainly from a conan stand - point . < br / > < br / > i was not expecting a great deal and thought they could not mess it up too much . . . . oh dear - how wrong was i . . . . < br / > < br / > the main flaw was it was fairly dull . it needed to zip along with a nice helping of supernatural going ##s - on , sword - fights and the like . < br / > < br / > you got some gore , but everything else was just pretty life - less [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1045 2001 2559 2830 2000 2023 17312 1012 2108 2019 2214 2728 1041 4922 5470 1010 3701 2013 1037 16608 3233 1011 2391 1012 1026 7987 1013 1028 1026 7987 1013 1028 1045 2001 2025 8074 1037 2307 3066 1998 2245 2027 2071 2025 6752 2009 2039 2205 2172 1012 1012 1012 1012 2821 6203 1011 2129 3308 2001 1045 1012 1012 1012 1012 1026 7987 1013 1028 1026 7987 1013 1028 1996 2364 28450 2001 2009 2001 7199 10634 1012 2009 2734 2000 14101 2247 2007 1037 3835 5094 1997 11189 2183 2015 1011 2006 1010 4690 1011 9590 1998 1996 2066 1012 1026 7987 1013 1028 1026 7987 1013 1028 2017 2288 2070 13638 1010 2021 2673 2842 2001 2074 3492 2166 1011 2625 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1045 2001 2559 2830 2000 2023 17312 1012 2108 2019 2214 2728 1041 4922 5470 1010 3701 2013 1037 16608 3233 1011 2391 1012 1026 7987 1013 1028 1026 7987 1013 1028 1045 2001 2025 8074 1037 2307 3066 1998 2245 2027 2071 2025 6752 2009 2039 2205 2172 1012 1012 1012 1012 2821 6203 1011 2129 3308 2001 1045 1012 1012 1012 1012 1026 7987 1013 1028 1026 7987 1013 1028 1996 2364 28450 2001 2009 2001 7199 10634 1012 2009 2734 2000 14101 2247 2007 1037 3835 5094 1997 11189 2183 2015 1011 2006 1010 4690 1011 9590 1998 1996 2066 1012 1026 7987 1013 1028 1026 7987 1013 1028 2017 2288 2070 13638 1010 2021 2673 2842 2001 2074 3492 2166 1011 2625 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    }
   ],
   "source": [
    "label_list = [0, 1]\n",
    "\n",
    "# We'll set sequences to be at most 128 tokens long.\n",
    "MAX_SEQ_LENGTH = 128\n",
    "# Convert our train and test features to InputFeatures that BERT understands.\n",
    "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpd\n",
   "language": "python",
   "name": "cpd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
